{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful libraries\n",
    "import sys\n",
    "import nltk # Vectorizer \n",
    "import sklearn # ML Library\n",
    "import os # file directories\n",
    "import spacy\n",
    "from statistics import mean\n",
    "from collections import Counter # counting most common words -- Not using anymore due to finding Vectorizer\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize) # Debugging to see the length of an array\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "stop_words = sp.Defaults.stop_words\n",
    "stop_words.add(\".\")\n",
    "stop_words.add(\",\")\n",
    "stop_words.add(\"'\")\n",
    "stop_words.add(\"``\")\n",
    "stop_words.add(\"''\")\n",
    "stop_words.add(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data importing & processing. Processing at import so there are less loops to process!\n",
    "business = []\n",
    "entertainment = []\n",
    "politics = []\n",
    "sport = []\n",
    "tech = []\n",
    "\n",
    "all_words = set()\n",
    "num_common_words = 5\n",
    "#business first\n",
    "for filename in os.listdir(\"bbc\\\\business\"):\n",
    "    file = open(os.path.join(\"bbc\\\\business\", filename),\"r\")\n",
    "    text = file.read()\n",
    "    char_count = len(text)\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    text_split_no_stop_words = [word.lower() for word in text if not word in stop_words]\n",
    "    common_words = [word for word, word_count in Counter(text_split_no_stop_words).most_common(num_common_words)]\n",
    "    avg_len = mean([len(i) for i in text])\n",
    "    data_set = []\n",
    "    data_set.extend(common_words)\n",
    "    data_set.append(len(text))\n",
    "    data_set.append(avg_len)\n",
    "    data_set.append(char_count)\n",
    "    business.append(data_set)\n",
    "    all_words.update(common_words)\n",
    "    del text\n",
    "    del common_words\n",
    "    del avg_len\n",
    "    del char_count\n",
    "    del data_set\n",
    "    file.close()\n",
    "\n",
    "#entertainment\n",
    "for filename in os.listdir(\"bbc\\\\entertainment\"):\n",
    "    file = open(os.path.join(\"bbc\\\\entertainment\", filename),\"r\")\n",
    "    text = file.read()\n",
    "    char_count = len(text)\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    text_split_no_stop_words = [word.lower() for word in text if not word in stop_words]\n",
    "    common_words = [word for word, word_count in Counter(text_split_no_stop_words).most_common(num_common_words)]\n",
    "    avg_len = mean([len(i) for i in text])\n",
    "    data_set = []\n",
    "    data_set.extend(common_words)\n",
    "    data_set.append(len(text))\n",
    "    data_set.append(avg_len)\n",
    "    data_set.append(char_count)\n",
    "    entertainment.append(data_set)\n",
    "    all_words.update(common_words)\n",
    "    del text\n",
    "    del common_words\n",
    "    del avg_len\n",
    "    del char_count\n",
    "    del data_set\n",
    "    file.close()\n",
    "    \n",
    "#politics\n",
    "for filename in os.listdir(\"bbc\\\\politics\"):\n",
    "    file = open(os.path.join(\"bbc\\\\politics\", filename),\"r\")\n",
    "    text = file.read()\n",
    "    char_count = len(text)\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    text_split_no_stop_words = [word.lower() for word in text if not word in stop_words]\n",
    "    common_words = [word for word, word_count in Counter(text_split_no_stop_words).most_common(num_common_words)]\n",
    "    avg_len = mean([len(i) for i in text])\n",
    "    data_set = []\n",
    "    data_set.extend(common_words)\n",
    "    data_set.append(len(text))\n",
    "    data_set.append(avg_len)\n",
    "    data_set.append(char_count)\n",
    "    politics.append(data_set)\n",
    "    all_words.update(common_words)\n",
    "    del text\n",
    "    del common_words\n",
    "    del avg_len\n",
    "    del char_count\n",
    "    del data_set\n",
    "    file.close()\n",
    "    \n",
    "#sport\n",
    "for filename in os.listdir(\"bbc\\\\sport\"):\n",
    "    file = open(os.path.join(\"bbc\\\\sport\", filename),\"r\")\n",
    "    text = file.read()\n",
    "    char_count = len(text)\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    text_split_no_stop_words = [word.lower() for word in text if not word in stop_words]\n",
    "    common_words = [word for word, word_count in Counter(text_split_no_stop_words).most_common(num_common_words)]\n",
    "    avg_len = mean([len(i) for i in text])\n",
    "    data_set = []\n",
    "    data_set.extend(common_words)\n",
    "    data_set.append(len(text))\n",
    "    data_set.append(avg_len)\n",
    "    data_set.append(char_count)\n",
    "    sport.append(data_set)\n",
    "    all_words.update(common_words)\n",
    "    del text\n",
    "    del common_words\n",
    "    del avg_len\n",
    "    del char_count\n",
    "    del data_set\n",
    "    file.close()\n",
    "\n",
    "#tech\n",
    "for filename in os.listdir(\"bbc\\\\tech\"):\n",
    "    file = open(os.path.join(\"bbc\\\\tech\", filename),\"r\")\n",
    "    text = file.read()\n",
    "    char_count = len(text)\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    text_split_no_stop_words = [word.lower() for word in text if not word in stop_words]\n",
    "    common_words = [word for word, word_count in Counter(text_split_no_stop_words).most_common(num_common_words)]\n",
    "    avg_len = mean([len(i) for i in text])\n",
    "    data_set = []\n",
    "    data_set.extend(common_words)\n",
    "    data_set.append(len(text))\n",
    "    data_set.append(avg_len)\n",
    "    data_set.append(char_count)\n",
    "    tech.append(data_set)\n",
    "    all_words.update(common_words)\n",
    "    del text\n",
    "    del common_words\n",
    "    del avg_len\n",
    "    del char_count\n",
    "    del data_set\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Training and Testing Data\n",
    "word_dict = dict()\n",
    "all_words = list(all_words) # This is done to preserve order as sets are un-ordered\n",
    "i = 1\n",
    "for word in all_words:\n",
    "    word_dict[word] = i # This gives \n",
    "    i += 1\n",
    "\n",
    "\n",
    "training_x = []\n",
    "training_y = []\n",
    "testing_x = []\n",
    "testing_y = []    \n",
    "    \n",
    "#Creating Training Data  \n",
    "for entry in range(0, len(business)-50):\n",
    "    data_converted = [word_dict[x] for x in business[entry][:num_common_words]]\n",
    "    data_converted.extend(business[entry][num_common_words:])\n",
    "    training_x.append(data_converted)\n",
    "    training_y.append(\"business\")\n",
    "    \n",
    "\n",
    "for entry in range(0, len(entertainment)-50):\n",
    "    data_converted = [word_dict[x] for x in entertainment[entry][:num_common_words]]\n",
    "    data_converted.extend(entertainment[entry][num_common_words:])\n",
    "    training_x.append(data_converted)\n",
    "    training_y.append(\"entertainment\")\n",
    "    \n",
    "\n",
    "for entry in range(0, len(politics)-50):\n",
    "    data_converted = [word_dict[x] for x in politics[entry][:num_common_words]]\n",
    "    data_converted.extend(politics[entry][num_common_words:])\n",
    "    training_x.append(data_converted)\n",
    "    training_y.append(\"politics\")\n",
    "    \n",
    "\n",
    "for entry in range(0, len(sport)-50):\n",
    "    data_converted = [word_dict[x] for x in sport[entry][:num_common_words]]\n",
    "    data_converted.extend(sport[entry][num_common_words:])\n",
    "    training_x.append(data_converted)\n",
    "    training_y.append(\"sport\")\n",
    "    \n",
    "\n",
    "for entry in range(0, len(tech)-50):\n",
    "    data_converted = [word_dict[x] for x in tech[entry][:num_common_words]]\n",
    "    data_converted.extend(tech[entry][num_common_words:])\n",
    "    training_x.append(data_converted)\n",
    "    training_y.append(\"tech\")\n",
    "    \n",
    "\n",
    "#Creating Testing Data\n",
    "for entry in range(len(business)-50,len(business)):\n",
    "    data_converted = [word_dict[x] for x in business[entry][:num_common_words]]\n",
    "    data_converted.extend(business[entry][num_common_words:])\n",
    "    testing_x.append(data_converted)\n",
    "    testing_y.append(\"business\")\n",
    "    \n",
    "\n",
    "for entry in range(len(entertainment)-50, len(entertainment)):\n",
    "    data_converted = [word_dict[x] for x in entertainment[entry][:num_common_words]]\n",
    "    data_converted.extend(entertainment[entry][num_common_words:])\n",
    "    testing_x.append(data_converted)\n",
    "    testing_y.append(\"entertainment\")\n",
    "    \n",
    "\n",
    "for entry in range(len(politics)-50, len(politics)):\n",
    "    data_converted = [word_dict[x] for x in politics[entry][:num_common_words]]\n",
    "    data_converted.extend(politics[entry][num_common_words:])\n",
    "    testing_x.append(data_converted)\n",
    "    testing_y.append(\"politics\")\n",
    "    \n",
    "\n",
    "for entry in range(len(sport)-50, len(sport)):\n",
    "    data_converted = [word_dict[x] for x in sport[entry][:num_common_words]]\n",
    "    data_converted.extend(sport[entry][num_common_words:])\n",
    "    testing_x.append(data_converted)\n",
    "    testing_y.append(\"sport\")\n",
    "    \n",
    "\n",
    "for entry in range(len(tech)-50, len(tech)):\n",
    "    data_converted = [word_dict[x] for x in tech[entry][:num_common_words]]\n",
    "    data_converted.extend(tech[entry][num_common_words:])\n",
    "    testing_x.append(data_converted)\n",
    "    testing_y.append(\"tech\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Machine Learning Time\n",
    "# THIS IS NOT USED\n",
    "\n",
    "\n",
    "# machine = sklearn.linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=510)\n",
    "# machine.fit(training_x, training_y)\n",
    "# print(machine.score(testing_x, testing_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST CLASSIFIER\n",
    "# NOT USED\n",
    "\n",
    "# from sklearn import ensemble\n",
    "# machine2 = ensemble.RandomForestClassifier()\n",
    "# machine2.fit(training_x, training_y)\n",
    "# print(machine2.score(testing_x, testing_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.428\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREE CLASSIFIER\n",
    "# NOT USED\n",
    "\n",
    "# from sklearn import tree\n",
    "# machine3 = tree.DecisionTreeClassifier()\n",
    "# machine3.fit(training_x, training_y)\n",
    "# print(machine3.score(testing_x, testing_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "# This is the chosen model\n",
    "\n",
    "machine4 = ensemble.GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "machine4.fit(training_x, training_y)\n",
    "# print(machine4.score(testing_x, testing_y))\n",
    "predictions_y = machine4.predict(testing_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.708\n",
      "macro precision: 0.7108543001655154\n",
      "macro recall: 0.708\n",
      "macro F1: 0.7076797852504535\n"
     ]
    }
   ],
   "source": [
    "#Gathering Metrics\n",
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(testing_y, predictions_y)\n",
    "macro_precision = metrics.precision_score(testing_y, predictions_y, average='macro')\n",
    "macro_f1 = metrics.f1_score(testing_y, predictions_y, average='macro')\n",
    "macro_recall = metrics.recall_score(testing_y, predictions_y, average='macro')\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"macro precision:\", macro_precision)\n",
    "print(\"macro recall:\", macro_recall)\n",
    "print(\"macro F1:\", macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
